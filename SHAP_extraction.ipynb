{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:01:17.415824968Z",
     "start_time": "2023-09-07T09:01:17.385981984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'shap_extractor' from '/home/rid/Projects/Study/Magister/Dyploma/shap_extractor.py'>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap_extractor as se\n",
    "from Bachelor.helpers import load_agent\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "from diploma.environment.plot import create_plots\n",
    "\n",
    "reload(se)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import Bachelor.rl as rl\n",
    "\n",
    "data_path = 'Bachelor/data/dataset_full.csv'\n",
    "sys.path.append(\"Bachelor/\")\n",
    "agent = load_agent('sac_last_60_50d_exp-r.pt', 'pt', 'Bachelor/agent')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T17:15:40.234493512Z",
     "start_time": "2023-09-06T17:15:40.187809772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess(data: pd.DataFrame):\n",
    "    df = data.copy()\n",
    "    cols = ['HR Usage Rate', 'TCH Blocking Rate, BH', 'Number of Available\\nTCH',\n",
    "               'TCH Traffic (Erl), BH', 'Lower_limit', 'Upper_limit']\n",
    "    df.drop(columns='DATA', inplace=True)\n",
    "    df.rename(columns={'Param 1': cols[-2], 'Param 2': cols[-1]}, inplace=True)\n",
    "\n",
    "    df = df[cols].drop(columns=['Cell ID', 'LAC'], errors='ignore')\n",
    "    df.rename_axis(None, axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df[cols]\n",
    "\n",
    "def cell_f(x):\n",
    "    res = agent.act(torch.as_tensor(x, dtype=torch.float32))\n",
    "\n",
    "    return res\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T17:15:46.162088453Z",
     "start_time": "2023-09-06T17:15:46.154162331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T17:22:15.161044133Z",
     "start_time": "2023-09-06T17:22:14.668095923Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "      Cell ID        DATA  Number of Available\\nTCH  HR Usage Rate  \\\n0       25771  2020-10-02                       4.0          94.95   \n1       25772  2020-10-02                       3.0          65.10   \n2       25773  2020-10-02                       3.0          97.38   \n3        3361  2020-10-02                       3.0          97.66   \n5        3363  2020-10-02                       3.0          31.84   \n...       ...         ...                       ...            ...   \n1028      882  2022-05-22                      13.0          78.00   \n1029      887  2022-05-22                      20.0          50.00   \n1030      883  2022-05-22                      12.0          88.00   \n1031      888  2022-05-22                      12.0          78.00   \n1032      884  2022-05-22                      12.0          61.00   \n\n      TCH Blocking Rate, BH  TCH Traffic (Erl), BH  Param 1  Param 2  \n0                      0.00                   7.78       56       68  \n1                      0.00                   5.74       24       43  \n2                      0.12                  13.42       78       92  \n3                      1.22                  18.27       90       97  \n5                      0.00                   5.10       36       50  \n...                     ...                    ...      ...      ...  \n1028                   0.00                   1.76       26       43  \n1029                   0.00                   8.76       12       21  \n1030                   0.26                   4.23       18       29  \n1031                   0.19                   8.04       19       31  \n1032                   0.00                   5.92       18       29  \n\n[588944 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cell ID</th>\n      <th>DATA</th>\n      <th>Number of Available\\nTCH</th>\n      <th>HR Usage Rate</th>\n      <th>TCH Blocking Rate, BH</th>\n      <th>TCH Traffic (Erl), BH</th>\n      <th>Param 1</th>\n      <th>Param 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25771</td>\n      <td>2020-10-02</td>\n      <td>4.0</td>\n      <td>94.95</td>\n      <td>0.00</td>\n      <td>7.78</td>\n      <td>56</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25772</td>\n      <td>2020-10-02</td>\n      <td>3.0</td>\n      <td>65.10</td>\n      <td>0.00</td>\n      <td>5.74</td>\n      <td>24</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25773</td>\n      <td>2020-10-02</td>\n      <td>3.0</td>\n      <td>97.38</td>\n      <td>0.12</td>\n      <td>13.42</td>\n      <td>78</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3361</td>\n      <td>2020-10-02</td>\n      <td>3.0</td>\n      <td>97.66</td>\n      <td>1.22</td>\n      <td>18.27</td>\n      <td>90</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3363</td>\n      <td>2020-10-02</td>\n      <td>3.0</td>\n      <td>31.84</td>\n      <td>0.00</td>\n      <td>5.10</td>\n      <td>36</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>882</td>\n      <td>2022-05-22</td>\n      <td>13.0</td>\n      <td>78.00</td>\n      <td>0.00</td>\n      <td>1.76</td>\n      <td>26</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>887</td>\n      <td>2022-05-22</td>\n      <td>20.0</td>\n      <td>50.00</td>\n      <td>0.00</td>\n      <td>8.76</td>\n      <td>12</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1030</th>\n      <td>883</td>\n      <td>2022-05-22</td>\n      <td>12.0</td>\n      <td>88.00</td>\n      <td>0.26</td>\n      <td>4.23</td>\n      <td>18</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1031</th>\n      <td>888</td>\n      <td>2022-05-22</td>\n      <td>12.0</td>\n      <td>78.00</td>\n      <td>0.19</td>\n      <td>8.04</td>\n      <td>19</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>1032</th>\n      <td>884</td>\n      <td>2022-05-22</td>\n      <td>12.0</td>\n      <td>61.00</td>\n      <td>0.00</td>\n      <td>5.92</td>\n      <td>18</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n<p>588944 rows Ã— 8 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T17:22:15.263107163Z",
     "start_time": "2023-09-06T17:22:15.261748311Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1043 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/597 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99891d031d8e4506ba37ea4929803306"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1043 [00:03<59:40,  3.44s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/597 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "482a6870a1194df4b3149e7a6f20af9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1043 [00:07<1:03:17,  3.65s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/597 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b7d160f4a2347809fa4229f212ae070"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1043 [00:10<1:00:14,  3.48s/it]"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/597 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0679b54bf804f4989dfc43a6ec7f5b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "expected_values = []\n",
    "path = 'data/cell/run_1/'\n",
    "\n",
    "for cell in tqdm(data['Cell ID'].unique()):\n",
    "    cell_df = data[data['Cell ID'] == cell]\n",
    "    cell_df = preprocess(cell_df)\n",
    "\n",
    "    path, exp_value = se.extract_shap(cell_df, cell_f, path, f'cell_{cell}')\n",
    "    expected_values.append((cell, exp_value))\n",
    "\n",
    "exp_df = pd.DataFrame(expected_values, columns=['cell_id', 'expected_value'])\n",
    "exp_df.to_csv(os.path.join(path, 'expected_values.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T18:01:52.067389977Z",
     "start_time": "2023-09-06T17:55:10.124197002Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 10, 6))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T17:44:03.719399576Z",
     "start_time": "2023-09-06T17:44:03.708139081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_11.parquet', -5094.744170488062], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_12.parquet', -1401.649470015138], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_13.parquet', -43.734246161038755], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_14.parquet', -3081.973257076359], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_15.parquet', -6144.5723419209535]]\n",
      "\n",
      "[['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_12.parquet', 10.502853701313143], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_13.parquet', 90.56149271692266], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_14.parquet', -190.3037156715145], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_15.parquet', -983.0158896500943], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_16.parquet', 183.61721836135257]]\n",
      "\n",
      "[['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_13.parquet', 34.24522927959333], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_14.parquet', -257.5884861716477], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_15.parquet', 228.0379354636534], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_16.parquet', -1813.602795117884], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_17.parquet', -2881.537827812557]]\n",
      "\n",
      "[['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_14.parquet', -2114.406451422459], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_15.parquet', -2622.72623642259], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_16.parquet', -3043.360811716324], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_17.parquet', -1930.7453895216458], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_18.parquet', 1872.5358845067967]]\n",
      "\n",
      "[['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_15.parquet', 6564.853419805848], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_16.parquet', 3085.565175061507], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_17.parquet', 1231.7195580951375], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_18.parquet', 5881.4621825026115], ['D:\\\\Studies\\\\diploma\\\\data\\\\1_m_timeframe\\\\2021_year\\\\week_19.parquet', 4587.07042240334]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_json = 'diploma/experiments/16_05_new_reward_window_size_16_ups_False/stop_loss_ne_100_rs_True_eer_False_ffh_0/profits_train/'\n",
    "list_of_jsons = glob.glob(path_to_json + '*.json')\n",
    "list_of_jsons = sorted(list_of_jsons)\n",
    "\n",
    "for json_file in list_of_jsons:\n",
    "    profits_dict = json.load(open(json_file, \"r\"))\n",
    "    print(profits_dict)\n",
    "    print()\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:10:56.223413285Z",
     "start_time": "2023-09-07T08:10:56.205595721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "infile = '/media/rid/Files/Datasets/Magister/2021_year/'\n",
    "use_predefined_scaler = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:11:02.287209951Z",
     "start_time": "2023-09-07T08:11:02.275696561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "list_of_weeks = [f'{infile}week_{i}.parquet' for i in range(10, 20, 1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:11:03.787355087Z",
     "start_time": "2023-09-07T08:11:03.783014501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['/media/rid/Files/Datasets/Magister/2021_year/week_10.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_11.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_12.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_13.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_14.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_15.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_16.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_17.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_18.parquet',\n '/media/rid/Files/Datasets/Magister/2021_year/week_19.parquet']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_weeks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:11:05.566598490Z",
     "start_time": "2023-09-07T08:11:05.554303573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10065/10065 [00:09<00:00, 1049.68it/s]\n",
      "You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from diploma.environment.crypto_env_random_price_new_reward import CryptoEnv\n",
    "\n",
    "if use_predefined_scaler:\n",
    "    train_env = False\n",
    "    if len(list_of_weeks) == 4:\n",
    "        path_to_scaler = f\"{infile}data_scalers_4_days/\"\n",
    "    elif len(list_of_weeks) == 6:\n",
    "        path_to_scaler = f\"{infile}data_scalers_6_days/\"\n",
    "    elif len(list_of_weeks) == 10:\n",
    "        path_to_scaler = f\"{infile}data_scalers_10_days/\"\n",
    "    elif len(list_of_weeks) == 15:\n",
    "        path_to_scaler = f\"{infile}data_scalers_15_days/\"\n",
    "else:\n",
    "    train_env = True\n",
    "    path_to_scaler = \"diploma/all_scalers/scalers_4/\"\n",
    "    Path(path_to_scaler).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'transaction_cost': 0.0001, #0.01\n",
    "    'window_size': 16,\n",
    "    'n_shares': 1,\n",
    "    'n_epochs': 100,\n",
    "    'train_env': train_env,\n",
    "    'random_start': True,\n",
    "    'df_path': infile,\n",
    "    'path_to_scaler': f\"{infile}data_scalers\\\\\",\n",
    "    'scaler_to_use': 'QuantileTransformer',\n",
    "    'initial_investment': 100000,\n",
    "    'end_episode_reward': False,\n",
    "    'add_metadata': True,\n",
    "    'fine_for_holding': 0\n",
    "}\n",
    "\n",
    "random_start = config['random_start']\n",
    "n_epochs_for_tune = 10\n",
    "\n",
    "config.update({'df_path': list_of_weeks[0],\n",
    "               'train_env': train_env,\n",
    "               'random_start': random_start})\n",
    "\n",
    "env = CryptoEnv(config)\n",
    "\n",
    "model_path = 'diploma/experiments/16_05_new_reward_window_size_16_ups_False/stop_loss_ne_100_rs_True_eer_False_ffh_0/ppo_models/ppo_model_trained_on_week_14.zip'\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:22:01.625533287Z",
     "start_time": "2023-09-07T08:21:51.647191229Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def trading_f(x):\n",
    "    act, _ = model.predict(x)\n",
    "\n",
    "    return act"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:00:12.105374692Z",
     "start_time": "2023-09-07T09:00:12.085467369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10065/10065 [00:09<00:00, 1054.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10063/10063 [00:10<00:00, 965.50it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10063 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac0981e1e7cc449a9c044b311fc6d2ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 90 iterations, alpha=6.938e-04, previous alpha=6.938e-04, with an active set of 47 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.616e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 158 iterations, alpha=8.012e-04, previous alpha=8.002e-04, with an active set of 47 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 168 iterations, alpha=5.392e-04, previous alpha=5.392e-04, with an active set of 43 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=1.005e-02, previous alpha=1.005e-02, with an active set of 41 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.220e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.160e-02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=4.368e-02, previous alpha=4.251e-02, with an active set of 29 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=9.496e-04, previous alpha=9.481e-04, with an active set of 48 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.074e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.068e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=5.517e-03, previous alpha=5.502e-03, with an active set of 36 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 47 iterations, alpha=1.128e-02, previous alpha=1.109e-02, with an active set of 34 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=7.113e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=7.118e-04, previous alpha=7.112e-04, with an active set of 45 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 57 iterations, alpha=4.122e-03, previous alpha=4.075e-03, with an active set of 38 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.205e-02, previous alpha=5.205e-02, with an active set of 15 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 52 iterations, alpha=1.819e-02, previous alpha=1.798e-02, with an active set of 37 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.502e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=5.053e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=5.053e-03, previous alpha=5.038e-03, with an active set of 42 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=4.081e-03, previous alpha=3.972e-03, with an active set of 41 regressors.\n",
      "Linear regression equation is singular, Moore-Penrose pseudoinverse is used instead of the regular inverse.\n",
      "To use regular inverse do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.680e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=7.757e-03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=5.043e-03, previous alpha=4.999e-03, with an active set of 48 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.105e-02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.021e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 74 iterations, alpha=5.970e-03, previous alpha=5.967e-03, with an active set of 39 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 150 iterations, alpha=3.170e-04, previous alpha=3.125e-04, with an active set of 47 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.629e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=1.555e-02, previous alpha=1.549e-02, with an active set of 38 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 52 iterations, alpha=9.959e-03, previous alpha=9.955e-03, with an active set of 33 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 160 iterations, alpha=1.067e-04, previous alpha=7.328e-05, with an active set of 49 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 97 iterations, alpha=2.504e-03, previous alpha=2.498e-03, with an active set of 38 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=2.084e-02, previous alpha=2.043e-02, with an active set of 25 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 55 iterations, alpha=1.242e-02, previous alpha=1.157e-02, with an active set of 34 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 93 iterations, alpha=3.686e-03, previous alpha=3.629e-03, with an active set of 46 regressors.\n",
      "Linear regression equation is singular, Moore-Penrose pseudoinverse is used instead of the regular inverse.\n",
      "To use regular inverse do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 100 iterations, alpha=1.506e-04, previous alpha=1.447e-04, with an active set of 45 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 45 iterations, alpha=2.003e-02, previous alpha=1.995e-02, with an active set of 32 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=4.143e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 110 iterations, alpha=2.070e-03, previous alpha=2.070e-03, with an active set of 43 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=8.992e-03, previous alpha=8.974e-03, with an active set of 42 regressors.\n",
      "Linear regression equation is singular, Moore-Penrose pseudoinverse is used instead of the regular inverse.\n",
      "To use regular inverse do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=2.598e-02, previous alpha=2.598e-02, with an active set of 26 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.971e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.991e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.575e-02, previous alpha=1.562e-02, with an active set of 28 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=7.410e-04, previous alpha=7.410e-04, with an active set of 48 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=4.124e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 97 iterations, alpha=2.010e-04, previous alpha=2.010e-04, with an active set of 42 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.406e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.395e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.496e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=1.267e-02, previous alpha=1.245e-02, with an active set of 36 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 126 iterations, alpha=1.526e-04, previous alpha=1.474e-04, with an active set of 45 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=5.835e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 133 iterations, alpha=3.197e-04, previous alpha=3.197e-04, with an active set of 48 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.340e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.514e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.394e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 69 iterations, alpha=7.064e-03, previous alpha=7.062e-03, with an active set of 40 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=1.376e-02, previous alpha=1.373e-02, with an active set of 33 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.292e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.343e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=2.334e-02, previous alpha=2.193e-02, with an active set of 24 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.716e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 166 iterations, alpha=4.144e-03, previous alpha=4.144e-03, with an active set of 43 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 118 iterations, alpha=1.416e-03, previous alpha=1.362e-03, with an active set of 49 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.136e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.554e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=6.535e-02, previous alpha=6.461e-02, with an active set of 16 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.310e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=4.970e-03, previous alpha=4.751e-03, with an active set of 42 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 63 iterations, alpha=8.845e-03, previous alpha=8.595e-03, with an active set of 38 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.506e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.751e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 115 iterations, alpha=1.627e-03, previous alpha=1.627e-03, with an active set of 46 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=1.445e-02, previous alpha=1.440e-02, with an active set of 37 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=6.146e-03, previous alpha=6.118e-03, with an active set of 42 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=5.036e-03, previous alpha=5.035e-03, with an active set of 34 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 90 iterations, alpha=2.845e-04, previous alpha=2.699e-04, with an active set of 47 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=8.345e-03, previous alpha=8.283e-03, with an active set of 38 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=1.166e-02, previous alpha=1.082e-02, with an active set of 35 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.781e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.395e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=3.263e-02, previous alpha=3.238e-02, with an active set of 17 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=1.243e-02, previous alpha=1.205e-02, with an active set of 32 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.202e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=1.745e-02, previous alpha=1.745e-02, with an active set of 28 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=2.158e-03, previous alpha=2.138e-03, with an active set of 35 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 91 iterations, alpha=2.366e-03, previous alpha=2.271e-03, with an active set of 42 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 94 iterations, alpha=2.891e-03, previous alpha=2.887e-03, with an active set of 41 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.675e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=1.646e-02, previous alpha=1.646e-02, with an active set of 30 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.906e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.657e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=3.657e-03, previous alpha=3.654e-03, with an active set of 37 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 125 iterations, alpha=7.492e-04, previous alpha=7.492e-04, with an active set of 44 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 59 iterations, alpha=6.686e-03, previous alpha=6.616e-03, with an active set of 34 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 113 iterations, alpha=2.906e-03, previous alpha=2.891e-03, with an active set of 48 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 122 iterations, alpha=2.126e-03, previous alpha=2.126e-03, with an active set of 49 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=3.257e-04, previous alpha=3.257e-04, with an active set of 48 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.961e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.785e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.474e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.462e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.454e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=2.462e-03, previous alpha=2.443e-03, with an active set of 31 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.070e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=3.689e-03, previous alpha=3.688e-03, with an active set of 46 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 45 iterations, alpha=2.535e-02, previous alpha=2.534e-02, with an active set of 30 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.613e-02, previous alpha=1.588e-02, with an active set of 31 regressors.\n",
      "Linear regression equation is singular, Moore-Penrose pseudoinverse is used instead of the regular inverse.\n",
      "To use regular inverse do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.681e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.675e-02, previous alpha=2.364e-02, with an active set of 24 regressors.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 73 iterations, alpha=7.008e-03, previous alpha=7.002e-03, with an active set of 46 regressors.\n",
      "Linear regression equation is singular, Moore-Penrose pseudoinverse is used instead of the regular inverse.\n",
      "To use regular inverse do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=3.910e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 109 iterations, alpha=3.841e-03, previous alpha=3.837e-03, with an active set of 42 regressors.\n"
     ]
    }
   ],
   "source": [
    "profits = []\n",
    "expected_values = []\n",
    "profits_path = 'data/trading/run_1/'\n",
    "Path(profits_path).mkdir(parents=True, exist_ok=True)\n",
    "path = profits_path\n",
    "\n",
    "for idw_test, week_test in enumerate(list_of_weeks[-5:]):\n",
    "    config.update({'df_path': week_test,\n",
    "                   'train_env': True,\n",
    "                   'random_start': False,\n",
    "                   'initial_investment': 70_000,})\n",
    "\n",
    "    env = CryptoEnv(config)\n",
    "\n",
    "    week_test_name = str(Path(week_test.replace('\\\\', '/')).name)[:7]\n",
    "\n",
    "    net_json = {0: [],\n",
    "                1: [],\n",
    "                2: [],\n",
    "                3: [],\n",
    "                4: [],\n",
    "                5: [],\n",
    "                6: [],}\n",
    "    net = []\n",
    "    actions = []\n",
    "    positions = []\n",
    "    share_prices = []\n",
    "    observations = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    for i in tqdm(range(len(env.states) - 2)):\n",
    "        action, _states = model.predict(obs)\n",
    "        actions.append(action)\n",
    "        observations.append(obs)\n",
    "\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        positions.append(env.position)\n",
    "        share_prices.append(env.current_price)\n",
    "\n",
    "        current_state_index, balance, own_share, net_worth, profit = env.render()\n",
    "        net_json[i // 1440].append(net_worth)\n",
    "        net.append(net_worth)\n",
    "\n",
    "        # if net_worth - config['initial_investment'] < -2000:\n",
    "        #     break\n",
    "\n",
    "    # Close the processes\n",
    "    env.close()\n",
    "\n",
    "    for i in range(7):\n",
    "        profit = net_json[i][-1] - net_json[i][0]\n",
    "        profits.append((week_test, i, profit))\n",
    "    #get shap\n",
    "    week_name = f\"week_{week_test[week_test.rfind('week_') + 5: week_test.rfind('.parquet')]}\"\n",
    "    path, exp_value = se.extract_shap(pd.DataFrame(observations), trading_f, path, week_name)\n",
    "    expected_values.append((week_name, exp_value))\n",
    "\n",
    "    # fig = create_plots(net, positions, actions, share_prices)\n",
    "    # fig.savefig(f'{plots_path}results_{week_test_name}.jpg', dpi=300)\n",
    "\n",
    "exp_df = pd.DataFrame(expected_values, columns=['week', 'expected_value'])\n",
    "exp_df.to_csv(os.path.join(path, 'expected_values.csv'))\n",
    "\n",
    "with open(profits_path + f'profits.json', 'w') as f:\n",
    "    f.write(json.dumps(profits))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-07T09:01:26.806096494Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
